{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zomata Stock Price Predicting using Time-Series analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU (Gated Recurrent Unit) is a recurrent neural network (RNN) that processes sequential input, such as time series or natural language processing. GRUs, like LSTMs, are intended to overcome the vanishing gradient problem that can arise while training standard RNNs. However, GRUs are simpler and have fewer parameters than LSTMs, making them quicker to train and less susceptible to overfitting.\n",
    "\n",
    "The GRU layer is imported from the tensorflow.keras.layers module, which is a component of the TensorFlow deep learning system. The GRU layer is used to generate a GRU model that can be trained using sequential data, such as stock price data. The GRU layer accepts an input shape, which determines the amount of time steps and features in the input data, and returns a series of hidden states that can then fed into a dense layer to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARCH Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An ARCH (Autoregressive Conditional Heteroskedasticity) model is a statistical method for modelling and forecasting time series data with time-varying volatility. Robert F. Engle introduced the ARCH model in 1982. The ARCH model is a form of GARCH (Generalised Autoregressive Conditional Heteroskedasticity) model, which is a broader family of models that enable the variance to be determined by previous variances and errors over a longer period of time. The ARCH model is used to calculate the variance of a time series as a function of previous variances and errors. The model is trained via maximum likelihood estimation (MLE) or another estimate approach.The ARCH model is used to model and forecast time series data that has variable volatility over time, such as stock prices, exchange rates, and interest rates. The ARCH model is a statistical model that is trained with the arch_model class in the arch Python package. The model is trained on previous time series data and then used to anticipate future data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU vs ARCH "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU (Gated Recurrent Unit) models and ARCH (Autoregressive Conditional Heteroskedasticity) models are both used for time series forecasting, although they operate on distinct principles and with different types of data.\n",
    "\n",
    "GRU models are recurrent neural networks (RNNs) that analyse sequential data, such as time series or natural language processing. GRU models forecast future stock values using previous data.\n",
    "\n",
    "ARCH models, on the other hand, are statistical models for modelling and forecasting time series data with variable volatility across time. The ARCH model is used to calculate the variance of a time series as a function of previous variances and errors.\n",
    "\n",
    "When it comes to forecasting stocks, GRU models offer some benefits over ARCH models:\n",
    "\n",
    "- GRU models are more adaptable and can capture more complicated patterns in data than ARCH models.\n",
    "- GRU models can accommodate missing and unevenly spaced data, but ARCH models require consistently spaced data.\n",
    "- GRU models may anticipate many steps ahead, whereas ARCH models are commonly employed for one-step forecasting.\n",
    "- GRU models can handle both univariate and multivariate time series data, whereas ARCH models are mainly utilised with univariate data.\n",
    "\n",
    "\n",
    "In conclusion, GRU models are more flexible and capable of capturing more complicated patterns in data than ARCH models. As a result, GRU models outperform ARCH models in terms of stock prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In data gathering, cleaning and processing. Here we are looking at systematically collecting information from various data sources, in that process we also identify the errors, inconsistencies and correcting them so that we can have data that is easy to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The fisrt step begins with importing all the necessary libraries. \n",
    "#This is done by importing the appropriate libraries from the appropriate repositories.\n",
    "\n",
    "import pandas as pd\n",
    "#from pandas import NaT\n",
    "import numpy as np\n",
    "\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying the data at its raw state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the cvs file \n",
    "df = pd.read_csv(r\"C:\\Users\\sibon\\Downloads\\zomato.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-07-23</td>\n",
       "      <td>116.000000</td>\n",
       "      <td>138.899994</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>694895290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-07-26</td>\n",
       "      <td>126.349998</td>\n",
       "      <td>143.750000</td>\n",
       "      <td>125.300003</td>\n",
       "      <td>140.649994</td>\n",
       "      <td>140.649994</td>\n",
       "      <td>249723854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-07-27</td>\n",
       "      <td>141.699997</td>\n",
       "      <td>147.800003</td>\n",
       "      <td>127.750000</td>\n",
       "      <td>132.899994</td>\n",
       "      <td>132.899994</td>\n",
       "      <td>240341900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-07-28</td>\n",
       "      <td>131.000000</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>123.550003</td>\n",
       "      <td>131.199997</td>\n",
       "      <td>131.199997</td>\n",
       "      <td>159793731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-07-29</td>\n",
       "      <td>134.949997</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>132.199997</td>\n",
       "      <td>141.550003</td>\n",
       "      <td>141.550003</td>\n",
       "      <td>117973089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>2024-02-01</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>138.550003</td>\n",
       "      <td>140.550003</td>\n",
       "      <td>140.550003</td>\n",
       "      <td>70252449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627</th>\n",
       "      <td>2024-02-02</td>\n",
       "      <td>141.800003</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>141.449997</td>\n",
       "      <td>143.800003</td>\n",
       "      <td>143.800003</td>\n",
       "      <td>78666454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>2024-02-05</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>145.399994</td>\n",
       "      <td>138.250000</td>\n",
       "      <td>140.250000</td>\n",
       "      <td>140.250000</td>\n",
       "      <td>54189688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>2024-02-06</td>\n",
       "      <td>140.399994</td>\n",
       "      <td>141.800003</td>\n",
       "      <td>138.050003</td>\n",
       "      <td>139.949997</td>\n",
       "      <td>139.949997</td>\n",
       "      <td>46782951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630</th>\n",
       "      <td>2024-02-07</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>142.899994</td>\n",
       "      <td>139.649994</td>\n",
       "      <td>140.399994</td>\n",
       "      <td>140.399994</td>\n",
       "      <td>75083259</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>631 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date        Open        High         Low       Close   Adj Close  \\\n",
       "0    2021-07-23  116.000000  138.899994  115.000000  126.000000  126.000000   \n",
       "1    2021-07-26  126.349998  143.750000  125.300003  140.649994  140.649994   \n",
       "2    2021-07-27  141.699997  147.800003  127.750000  132.899994  132.899994   \n",
       "3    2021-07-28  131.000000  135.000000  123.550003  131.199997  131.199997   \n",
       "4    2021-07-29  134.949997  144.000000  132.199997  141.550003  141.550003   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "626  2024-02-01  141.000000  143.500000  138.550003  140.550003  140.550003   \n",
       "627  2024-02-02  141.800003  145.000000  141.449997  143.800003  143.800003   \n",
       "628  2024-02-05  145.000000  145.399994  138.250000  140.250000  140.250000   \n",
       "629  2024-02-06  140.399994  141.800003  138.050003  139.949997  139.949997   \n",
       "630  2024-02-07  141.000000  142.899994  139.649994  140.399994  140.399994   \n",
       "\n",
       "        Volume  \n",
       "0    694895290  \n",
       "1    249723854  \n",
       "2    240341900  \n",
       "3    159793731  \n",
       "4    117973089  \n",
       "..         ...  \n",
       "626   70252449  \n",
       "627   78666454  \n",
       "628   54189688  \n",
       "629   46782951  \n",
       "630   75083259  \n",
       "\n",
       "[631 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()\n",
    "#no null values found in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary of the data set.\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using skimpy to get a more detailed description of the data set.\n",
    "\n",
    "import skimpy as sk\n",
    "sk.skim(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert date Strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet defines lists for days and months and provides a function to convert date strings into a structured format. This type of code is essential for handling dates effectively in various applications. It allows us to work with dates in a structured manner, making our code more robust and insightful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = [\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\",\"Sunday\"]\n",
    "months = [\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"]\n",
    "def convert_dates(x):\n",
    "    date = datetime.datetime.strptime(x, \"%Y-%m-%d\")\n",
    "    return [date.year, date.month, date.day, date.isoweekday()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code transforms date strings into structured components (year, month, day, and weekday) and adds them as new columns to the DataFrame. These new columns can be useful for further analysis or visualization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Year\"] = df[\"Date\"].apply(lambda x: convert_dates(x)[0])\n",
    "df[\"Month\"] = df[\"Date\"].apply(lambda x: months[convert_dates(x)[1]-1])\n",
    "df[\"Day\"] = df[\"Date\"].apply(lambda x: convert_dates(x)[2])\n",
    "df[\"Weekday\"] = df[\"Date\"].apply(lambda x: days[convert_dates(x)[3]-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loop will create line plots for each column from the second to the seventh column.\n",
    " for i in df.columns[1:7]:: This is a loop that iterates over column names in the DataFrame df starting from the second column (df.columns[1]) up to, but not including, the seventh column (df.columns[7]).\n",
    "fig = px.line(df, x=\"Date\", y=i, color=\"Year\"): Inside the loop, a line plot is created using Plotly Express (px). The x parameter specifies the data for the x-axis, which is the \"Date\" column of the DataFrame. The y parameter specifies the data for the y-axis, which is the column currently being iterated over (i). The color parameter specifies how the lines will be colored, based on the \"Year\" column of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df.columns [1 :7]:\n",
    "    fig = px.line(df, x=\"Date\", y=i , color= \"Year\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with the outliers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identifying the range values for each column in the data.\n",
    "df = df.describe([x*0.1 for x in range (10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying the boxplot showing the the distribution of the outliers.\n",
    "import seaborn as sns\n",
    "sns.boxplot(x=df1['Open'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Breakdown\n",
    "\n",
    "### Data Preperation(prep_data_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to prep csv file\n",
    "def prepare_data():\n",
    "    # Read stock price data\n",
    "    url = 'https://raw.githubusercontent.com/Tiaan-Botes/CYO_Project_Group_E/52663ba4e6833e232f1bbd7a0ab48edb23f52b91/data/data.csv'\n",
    "    data = pd.read_csv(url)\n",
    "\n",
    "    data['Date'] = pd.to_datetime(data['Date'])\n",
    "    data.sort_values('Date', inplace=True)\n",
    "    data.set_index('Date', inplace=True)\n",
    "    data.drop(columns=['Unnamed: 0', 'Year', 'Month', 'Day', 'Weekday'], inplace=True)\n",
    "\n",
    "    data['ma_30'] = data['Adj Close'].rolling(window=30).mean()\n",
    "    data['ma_90'] = data['Adj Close'].rolling(window=90).mean()\n",
    "    \n",
    "    data['daily_returns'] = data['Adj Close'].pct_change()*100\n",
    "    data.dropna(inplace=True)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The programme begins by retrieving stock price information from a CSV file available on GitHub. This information most likely includes historical stock values for a certain firm.\n",
    "\n",
    "It then processes the data.\n",
    "- Converts the 'Date' column to datetime format and uses it as an index. This step makes the date the main identifier for each data entry.\n",
    "- Sorts the data by date to maintain chronological order.\n",
    "- Removes unnecessary columns like 'Unnamed: 0', 'Year', 'Month', 'Day', and 'Weekday'. These columns can be superfluous or irrelevant to the study.\n",
    "- For the 'Adj Close' column, two moving averages (MA) are calculated across various time frames (30 and 90 days). Moving averages are often used to smooth out volatility and show long-term patterns in data.\n",
    "- Calculates daily returns using the percentage change in the 'Adj Close' column from one day to the next. This measure gives insight into the stock's daily volatility or performance.\n",
    "- Removes rows with missing values to ensure data integrity.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = prepare_data()\n",
    "df.info()\n",
    "print(df.head())\n",
    "\n",
    "plot_data = df.loc['2023-01-01':'2023-12-31']\n",
    "\n",
    "# Plotting stock price along with moving averages\n",
    "plt.figure(figsize=(20, 25))\n",
    "fig = px.line(\n",
    "    data_frame=plot_data, \n",
    "    x=plot_data.index, \n",
    "    y=['Adj Close', 'ma_30', 'ma_90']\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Plotting daily returns\n",
    "plt.figure(figsize=(10, 25))\n",
    "fig = px.line(\n",
    "    data_frame=df, \n",
    "    x=df.index, \n",
    "    y=['daily_returns']\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "After preprocessing the data, the code creates visualisations to gather insights:\n",
    "- The first chart depicts the stock's adjusted closing price, as well as its 30-day and 90-day moving averages. This visualisation depicts the trend and fluctuation of the stock price over time.\n",
    "- The second plot depicts the stock's daily returns, allowing analysts to discover times of significant volatility or unexpected price moves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU Model Preparation and Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to prepare data for GRU model\n",
    "def prepare_gru_data():\n",
    "    data = prepare_data()  \n",
    "    dataset = data[['Adj Close']].values.astype('float32')\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    dataset = scaler.fit_transform(dataset)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# Function to build GRU model\n",
    "def build_gru_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(units=50, input_shape=input_shape))\n",
    "    model.add(Dense(units=1))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    return model\n",
    "\n",
    "# Function to train GRU model\n",
    "def train_gru_model(dataset):\n",
    "    train_size = int(len(dataset) * 0.67)\n",
    "    test_size = len(dataset) - train_size\n",
    "    train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "\n",
    "    def create_dataset(dataset, look_back=1):\n",
    "        X, Y = [], []\n",
    "        for i in range(len(dataset)-look_back-1):\n",
    "            a = dataset[i:(i+look_back), 0]\n",
    "            X.append(a)\n",
    "            Y.append(dataset[i + look_back, 0])\n",
    "\n",
    "        return np.array(X), np.array(Y)\n",
    "\n",
    "    look_back = 1\n",
    "    trainX, trainY = create_dataset(train, look_back)\n",
    "    testX, testY = create_dataset(test, look_back)\n",
    "\n",
    "    trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "    testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
    "\n",
    "    model = build_gru_model((1, look_back))\n",
    "    model.fit(trainX, trainY, epochs=100, batch_size=1, verbose=2)\n",
    "\n",
    "    return model, testX, testY\n",
    "# Function to make predictions using GRU model\n",
    "def predict_gru_model(model, testX):\n",
    "    return model.predict(testX)\n",
    "\n",
    "# Preparing data\n",
    "gru_dataset = prepare_gru_data()\n",
    "\n",
    "# Training \n",
    "gru_model, testX, testY = train_gru_model(gru_dataset)\n",
    "\n",
    "# predictions\n",
    "gru_predictions = predict_gru_model(gru_model, testX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm then prepares data for training a Gated Recurrent Unit (GRU) model, \n",
    "The prepare_gru_data method pulls the 'Adj Close' column from the generated DataFrame and scales it using MinMaxScaler, transforming the data into a range of 0 to 1. Scaling guarantees that all input characteristics contribute equally to model training while preventing any one element from overpowering the others.\n",
    "\n",
    "\n",
    "Next, the code specifies functions for constructing, training, and utilising the GRU model.\n",
    "The build_gru_model method creates a Sequential Neural Network model with TensorFlow's Keras API. It is made up of a GRU layer with 50 units, followed by a Dense layer with one unit. The model is built using the Adam optimizer and mean squared error loss, both of which are popular methods for training regression models.\n",
    "\n",
    "The train_gru_model function divides the dataset into training and testing sets, generates input-output pairs by shifting the time series data with a defined look-back window, reshapes the input data to fit the GRU model's input criteria, constructs the model, and trains it on the training data. The model is trained for 100 epochs(pass throughs), with a batch size of one.\n",
    "\n",
    "The predict_gru_model function uses the trained GRU model and input data to make predictions for the test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Assessment:\n",
    "\n",
    "Finally, the algorithm assesses the performance of the GRU model and displays the outcomes:\n",
    "\n",
    "It estimates the Root Mean Squared Error (RMSE) of the actual and anticipated values. The RMSE is a measure of the model's prediction accuracy that shows the average size of the mistakes.\n",
    "\n",
    "The model's performance is graphically compared by plotting the actual and projected numbers. This enables analysts to determine how well the model reflects the data's underlying trends and if it can accurately forecast future stock values.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machinelearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
